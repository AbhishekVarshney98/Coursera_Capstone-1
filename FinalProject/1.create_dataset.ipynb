{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a World Cities dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wikipedia page [List of largest cities](https://en.wikipedia.org/wiki/List_of_largest_cities) has a list of the largest cities in the world.  \n",
    "We are interested in table that contains the actual cities, and want the cityname, the nation, and its city proper population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Section 2](#2.-Scrape-list-of-largest-cities-from-Wikipedia), we use urllib to fetch the page, then BeautifulSoup to parse it and find the first (and only) sortable table.  \n",
    "We also scrape the URL referencing the individual cities' pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Section 3](#3.-Adding-geopositioning-data) we scrape the wikipedia page for every individual city in order to get the cities geographic coordinates.  \n",
    "In theory we could use this step to grab more information, like population density. However, since we want to demonstrate the use of the Foursquare API, we will not use such data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Section 4](#4.-Getting-more-information-from-Foursquare) we use the Foursquare API to learn more about these cities. \n",
    "Specifically we ask for top recommendations in those cities, to get an idea of what is popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Section 5](#5.-Saving-the-dataframes) we export the created dataframe so we can import it in other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and such things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following libraries to fetch and parse html pages, and to interact with the Foursquare API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use regular expressions, e.g. to extract population count and city coordinates from scraped webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Pandas for building the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrape list of largest cities from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code fetches a Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the wikipedia article\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_largest_cities\"\n",
    "\n",
    "# fetch the article\n",
    "req = urllib.request.urlopen(url)\n",
    "article = req.read().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use BeautifulSoup to parse the obtained HTML, and find the first sortable table. This is the table of Largest cities that dominates the Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse with BeautifulSoup and find the first sortable table\n",
    "soup = BeautifulSoup(article, 'html.parser')\n",
    "table = soup.find('table', class_='sortable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new Pandas dataframe to hold our cities. We will also save its nation, its population count, and its URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Population, dtype: int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an empty DataFrame\n",
    "cols=[\"City\", \"Nation\", \"Population\", \"URL\"]\n",
    "df_cities = pd.DataFrame(columns=cols)\n",
    "df_cities['Population'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now traverse through the entire table and append the required data to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate trough all the rows in the table:\n",
    "for tr in table.find_all('tr'):\n",
    "    tds = tr.find_all('td')\n",
    "    if not tds:\n",
    "        continue                            # skips first row with headings\n",
    "    nation = tds[0].find('a').string        # first td column contains nation   \n",
    "    try:\n",
    "        pop = int(re.compile(r'\\[.*\\]').sub(\"\",tds[2].text).replace(',',''))  # rough but working way to parse the population count\n",
    "    except ValueError:\n",
    "        pop = 0                             # but not every city has a population count\n",
    "    city_a = tr.find('th').find('a')        # the first column contains th tag and contains the <a> link to the city\n",
    "    city = city_a.string\n",
    "    url = \"https://en.wikipedia.org\" + city_a['href']\n",
    "    df_cities = df_cities.append({\n",
    "        'City': str(city), \n",
    "        'Nation': str(nation), \n",
    "        'Population': pop, \n",
    "        'URL': str(url)\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: {} (247, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Nation</th>\n",
       "      <th>Population</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chongqing</td>\n",
       "      <td>China</td>\n",
       "      <td>30751600</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Chongqing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shanghai</td>\n",
       "      <td>China</td>\n",
       "      <td>24256800</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Shanghai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>India</td>\n",
       "      <td>11034555</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "      <td>21516000</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Beijing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dhaka</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>14399000</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dhaka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City      Nation Population                                      URL\n",
       "0  Chongqing       China   30751600  https://en.wikipedia.org/wiki/Chongqing\n",
       "1   Shanghai       China   24256800   https://en.wikipedia.org/wiki/Shanghai\n",
       "2      Delhi       India   11034555      https://en.wikipedia.org/wiki/Delhi\n",
       "3    Beijing       China   21516000    https://en.wikipedia.org/wiki/Beijing\n",
       "4      Dhaka  Bangladesh   14399000      https://en.wikipedia.org/wiki/Dhaka"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Shape: {}\", df_cities.shape)\n",
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding geopositioning data\n",
    "Every city's wikipedia page contains the geographic coordinates, which we can also scrape.\n",
    "The following function scrapes the city page and uses a simple regular expression to capture the coordinates.\n",
    "\n",
    "Here we don't need BeautifulSoup, since we have a Regular Expression that we can directly run on the fetched html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape an individual cities page for its coordinates\n",
    "def scrape_city_coords(url):\n",
    "    req = urllib.request.urlopen(url)\n",
    "    article = req.read().decode()\n",
    "    reg = re.search(r'\"lat\":(.*?),\"lon\":(.*?)}', article)\n",
    "    lat = float(reg.group(1))\n",
    "    lon = float(reg.group(2))\n",
    "    return lat,lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will run this function on the URLs to create two new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities[\"Latitude\"], df_cities[\"Longitude\"] = zip(*df_cities[\"URL\"].map(scrape_city_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Nation</th>\n",
       "      <th>Population</th>\n",
       "      <th>URL</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chongqing</td>\n",
       "      <td>China</td>\n",
       "      <td>30751600</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Chongqing</td>\n",
       "      <td>29.558333</td>\n",
       "      <td>106.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shanghai</td>\n",
       "      <td>China</td>\n",
       "      <td>24256800</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Shanghai</td>\n",
       "      <td>31.228611</td>\n",
       "      <td>121.474722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>India</td>\n",
       "      <td>11034555</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Delhi</td>\n",
       "      <td>28.610000</td>\n",
       "      <td>77.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "      <td>21516000</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Beijing</td>\n",
       "      <td>39.916667</td>\n",
       "      <td>116.383333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dhaka</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>14399000</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dhaka</td>\n",
       "      <td>23.716111</td>\n",
       "      <td>90.396111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City      Nation Population                                      URL  \\\n",
       "0  Chongqing       China   30751600  https://en.wikipedia.org/wiki/Chongqing   \n",
       "1   Shanghai       China   24256800   https://en.wikipedia.org/wiki/Shanghai   \n",
       "2      Delhi       India   11034555      https://en.wikipedia.org/wiki/Delhi   \n",
       "3    Beijing       China   21516000    https://en.wikipedia.org/wiki/Beijing   \n",
       "4      Dhaka  Bangladesh   14399000      https://en.wikipedia.org/wiki/Dhaka   \n",
       "\n",
       "    Latitude   Longitude  \n",
       "0  29.558333  106.566667  \n",
       "1  31.228611  121.474722  \n",
       "2  28.610000   77.230000  \n",
       "3  39.916667  116.383333  \n",
       "4  23.716111   90.396111  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Getting more information from Foursquare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information is needed to connect with Foursquare API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'YPBVFDUZOP1M24BKCWGXIYZ3RFACOE3V35WSFY4DSCMRU44L' # your Foursquare ID\n",
    "CLIENT_SECRET = 'VYHYTBSRIZBPYAOCP5ZEFV3YM4C40YEQCQWCUO4NC1JTPNJM' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ask Foursquare for the top picks in every city, then store its name, location and main category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecommendedVenues(cities, latitudes, longitudes):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for city, lat, lon in zip(cities, latitudes, longitudes):\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&section=topPicks&client_id={}&client_secret={}&v={}&ll={},{}&limit=50'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lon)\n",
    "            \n",
    "        # make the GET request\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == requests.codes.ok:\n",
    "            results = response.json()[\"response\"]['groups'][0]['items']\n",
    "        else:\n",
    "            print (\"status was:\" + str(response.status_code))\n",
    "            print (response)\n",
    "            continue\n",
    "        \n",
    "        print(\"City: {}, results: {}\".format(city, len(results)))\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            city,\n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['City',\n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, actually run the above function for every city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: Chongqing, results: 18\n",
      "City: Shanghai, results: 41\n",
      "City: Delhi, results: 50\n",
      "City: Beijing, results: 42\n",
      "City: Dhaka, results: 30\n",
      "City: Mumbai, results: 37\n",
      "City: Lagos, results: 50\n",
      "City: Chengdu, results: 24\n",
      "City: Karachi, results: 50\n",
      "City: Guangzhou, results: 50\n",
      "City: Istanbul, results: 50\n",
      "City: Tokyo, results: 45\n",
      "City: Tianjin, results: 32\n",
      "City: Moscow, results: 50\n",
      "City: São Paulo, results: 50\n",
      "City: Kinshasa, results: 22\n",
      "City: Baoding, results: 2\n",
      "City: Lahore, results: 50\n",
      "City: Cairo, results: 50\n",
      "City: Seoul, results: 6\n",
      "City: Jakarta, results: 19\n",
      "City: Wenzhou, results: 9\n",
      "City: Mexico City, results: 50\n",
      "City: Lima, results: 50\n",
      "City: London, results: 50\n",
      "City: Bangkok, results: 26\n",
      "City: Xi'an, results: 24\n",
      "City: Chennai, results: 50\n",
      "City: Bangalore, results: 50\n",
      "City: New York City, results: 50\n",
      "City: Ho Chi Minh City, results: 30\n",
      "City: Hyderabad, results: 50\n",
      "City: Shenzhen, results: 50\n",
      "City: Suzhou, results: 21\n",
      "City: Nanjing, results: 27\n",
      "City: Dongguan, results: 21\n",
      "City: Tehran, results: 50\n",
      "City: Quanzhou, results: 5\n",
      "City: Shenyang, results: 18\n",
      "City: Bogotá, results: 50\n",
      "City: Hong Kong, results: 36\n",
      "City: Baghdad, results: 50\n",
      "City: Fuzhou, results: 12\n",
      "City: Changsha, results: 10\n",
      "City: Wuhan, results: 8\n",
      "City: Hanoi, results: 50\n",
      "City: Rio de Janeiro, results: 50\n",
      "City: Qingdao, results: 25\n",
      "City: Foshan, results: 11\n",
      "City: Zunyi, results: 8\n",
      "City: Santiago, results: 34\n",
      "City: Riyadh, results: 50\n",
      "City: Ahmedabad, results: 50\n",
      "City: Singapore, results: 6\n",
      "City: Shantou, results: 4\n",
      "City: Ankara, results: 50\n",
      "City: Yangon, results: 50\n",
      "City: Saint Petersburg, results: 50\n",
      "City: Sydney, results: 13\n",
      "City: Casablanca, results: 50\n",
      "City: Melbourne, results: 24\n",
      "City: Abidjan, results: 50\n",
      "City: Alexandria, results: 50\n",
      "City: Kolkata, results: 50\n",
      "City: Surat, results: 50\n",
      "City: Johannesburg, results: 50\n",
      "City: Dar es Salaam, results: 50\n",
      "City: Shijiazhuang, results: 5\n",
      "City: Harbin, results: 14\n",
      "City: Giza, results: 50\n",
      "City: İzmir, results: 50\n",
      "City: Zhengzhou, results: 4\n",
      "City: New Taipei City, results: 12\n",
      "City: Los Angeles, results: 26\n",
      "City: Changchun, results: 7\n",
      "City: Cape Town, results: 50\n",
      "City: Yokohama, results: 18\n",
      "City: Khartoum, results: 28\n",
      "City: Guayaquil, results: 50\n",
      "City: Hangzhou, results: 46\n",
      "City: Xiamen, results: 25\n",
      "City: Berlin, results: 50\n",
      "City: Busan, results: 14\n",
      "City: Ningbo, results: 26\n",
      "City: Jeddah, results: 50\n",
      "City: Durban, results: 35\n",
      "City: Algiers, results: 50\n",
      "City: Kabul, results: 17\n",
      "City: Hefei, results: 8\n",
      "City: Mashhad, results: 50\n",
      "City: Pyongyang, results: 4\n"
     ]
    }
   ],
   "source": [
    "df_venues = getRecommendedVenues(df_cities['City'], df_cities['Latitude'], df_cities['Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's looking pretty awesome. Now let's prevent more scraping by saving the dataframe to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Joining the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = pd.merge(df_venues, df_cities, on='City')\n",
    "df_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export the dataframes so we don't have to scrape again.  \n",
    "First as CSV, as this is a very generic format.  \n",
    "Secondly as pickle, since this is a quick way to import the dataframe again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export both dataframes as CSV\n",
    "df_cities.to_csv('cities.csv')\n",
    "df_venues.to_csv('venues.csv')\n",
    "df_joined.to_csv('joined.cvs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export both dataframe as pickles\n",
    "df_cities.to_pickle('cities.pickle')\n",
    "df_venues.to_pickle('venues.pickle')\n",
    "df_joined.to_pickle('joined.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
